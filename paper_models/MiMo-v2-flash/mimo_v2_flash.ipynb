{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8ba4608f",
   "metadata": {},
   "source": [
    "**MiMo-V2-Flash核心设计**\n",
    "1. 混合注意力: 128-token滑动窗口注意力(SWA) + 全局注意力(GA), ratio = 5:1\n",
    "2. MoE 结构: 256个专家，每 token 激活 8 个\n",
    "3. 轻量 MTP 模块: 稠密 FFN + SWA， 用于投机解码加速\n",
    "4. 关键优化: RoPE位置编码、RMSNorm、FP16 混合精度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "49395fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载依赖库\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from math import sqrt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "91b95d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RMSNorm\n",
    "class RMSNorm(nn.Module):\n",
    "    \"\"\"RMSNorm 归一化\"\"\"\n",
    "    def __init__(self, dim: int, eps: float = 1e-6):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.weight = nn.Parameter(torch.ones(dim))\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps) * self.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "00dc1f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RoPE位置编码\n",
    "class RoPE(nn.Module):\n",
    "    \"\"\"旋转位置编码（仅应用前 64 维）\"\"\"\n",
    "    def __init__(self, dim: int, max_seq_len: int = 256000):\n",
    "        super().__init__()\n",
    "        self.max_seq_len = max_seq_len\n",
    "        # 仅对前 64 维应用 RoPE\n",
    "        self.rope_dim = min(64, dim)\n",
    "        theta = 1.0 / (10000 ** (torch.arange(0, self.rope_dim, 2) / self.rope_dim))\n",
    "        self.register_buffer(\"theta\", theta)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        seq_len = x.shape[1]\n",
    "        device = x.device\n",
    "        # 生成位置索引\n",
    "        pos = torch.arange(seq_len, device=device).unsqueeze(1)\n",
    "        # 计算旋转矩阵参数\n",
    "        rope = pos * self.theta.unsqueeze(0)\n",
    "        rope = torch.cat([rope, rope], dim=-1)\n",
    "        # 仅对前 rope_dim 维应用旋转\n",
    "        if self.rope_dim < self.dim:\n",
    "            rope = torch.cat([rope, torch.zeros(seq_len, self.dim - self.rope_dim, device=device)], dim=-1)\n",
    "        # 构建旋转矩阵并应用\n",
    "        cos = rope.cos().unsqueeze(0)\n",
    "        sin = rope.sin().unsqueeze(0)\n",
    "        x_rope = x[..., :self.rope_dim]\n",
    "        x_rope = torch.cat([x_rope[..., ::2] * cos[..., ::2] - x_rope[..., 1::2] * sin[..., 1::2],\n",
    "                           x_rope[..., 1::2] * cos[..., ::2] + x_rope[..., ::2] * sin[..., 1::2]], dim=-1)\n",
    "        if self.rope_dim < self.dim:\n",
    "            x = torch.cat([x_rope, x[..., self.rope_dim:]], dim=-1)\n",
    "        else:\n",
    "            x = x_rope\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4b166f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expert\n",
    "class Expert(nn.Module):\n",
    "    \"\"\"MoE 单个专家网络(稠密 FFN)\"\"\"\n",
    "    def __init__(self, dim: int, hidden_dim: int = 2048):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, dim)\n",
    "        self.act = F.silu\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.fc2(self.act(self.fc1(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1a17437c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MoEFFN\n",
    "class MoEFFN(nn.Module):\n",
    "    \"\"\"MoE 前馈网络(256专家，激活 8 个)\"\"\"\n",
    "    def __init__(self, dim: int, num_experts: int = 256, top_k: int = 8):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.num_experts = num_experts\n",
    "        self.top_k = top_k\n",
    "        # 专家网络\n",
    "        self.experts = nn.ModuleList([Expert(dim) for _ in range(num_experts)])\n",
    "        # 专家理由\n",
    "        self.router = nn.Linear(dim, num_experts)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        batch_size, seq_len, dim = x.shape\n",
    "        x_flat = x.reshape(-1, dim)  # [batch_size * seq_len, dim]\n",
    "\n",
    "        # 路由选择专家\n",
    "        router_logits = self.router(x_flat)  # [batch_size * seq_len, num_experts]\n",
    "        top_k_logits, top_k_indices = torch.topk(router_logits, self.top_k, dim=-1)  # [batch_size * seq_len, top_k]\n",
    "        top_k_weights = F.softmax(top_k_logits, dim=-1)  # [batch_size * seq_len, top_k]\n",
    "\n",
    "        # 收集专家输出\n",
    "        output = torch.zeros_like(x_flat)\n",
    "        for i in range(self.top_k):\n",
    "            expert_idx = top_k_indices[:, i]\n",
    "            weight = top_k_weights[:, i].unsqueeze(-1)  # [batch_size * seq_len, 1]\n",
    "            # 按专家分组计算\n",
    "            for expert_id in range(self.num_experts):\n",
    "                mask = (expert_idx == expert_id)\n",
    "                if mask.any():\n",
    "                    expert_output = self.experts[expert_id](x_flat[mask])\n",
    "                    output[mask] += weight[mask] * expert_output\n",
    "\n",
    "        return output.reshape(batch_size, seq_len, dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "95ad8a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SlidingWindowAttention\n",
    "class SlidingWindowAttention(nn.Module):\n",
    "    \"\"\"滑动窗口注意力(SWA)\"\"\"\n",
    "    def __init__(self, dim: int, num_heads: int = 64, num_kv_heads: int = 8, window_size: int = 128):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.num_heads = num_heads\n",
    "        self.num_kv_heads = num_kv_heads\n",
    "        self.head_dim = dim // num_heads\n",
    "        self.window_size = window_size\n",
    "        # 注意力 sink 偏置\n",
    "        self.sink_bias = nn.Parameter(torch.tensor(0.0))\n",
    "\n",
    "        # QKV 投影\n",
    "        self.q_proj = nn.Linear(dim, num_heads * self.head_dim)\n",
    "        self.k_proj = nn.Linear(dim, num_kv_heads * self.head_dim)\n",
    "        self.v_proj = nn.Linear(dim, num_kv_heads * self.head_dim)\n",
    "        self.o_proj = nn.Linear(num_heads * self.head_dim, dim)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor, mask: torch.Tensor = None) -> torch.Tensor:\n",
    "        batch_size, seq_len, dim = x.shape\n",
    "\n",
    "        # QKV 投影\n",
    "        q = self.q_proj(x).reshape(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)  # [batch_size, num_heads, seq_len, head_dim]\n",
    "        k = self.k_proj(x).reshape(batch_size, seq_len, self.num_kv_heads, self.head_dim).transpose(1, 2)  # [batch_size, num_kv_heads, seq_len, head_dim]\n",
    "        v = self.v_proj(x).reshape(batch_size, seq_len, self.num_kv_heads, self.head_dim).transpose(1, 2)  # [batch_size, num_kv_heads, seq_len, head_dim]\n",
    "\n",
    "        # 重复 KV 头以匹配 Q 头数量（GQA 机制）\n",
    "        if self.num_kv_heads != self.num_heads:\n",
    "            k = k.repeat_interleave(self.num_heads // self.num_kv_heads, dim=1)\n",
    "            v = v.repeat_interleave(self.num_heads // self.num_kv_heads, dim=1)\n",
    "        \n",
    "        # 滑动窗口注意力计算\n",
    "        attn_weights = torch.matmul(q, k.transpose(-2, -1)) / sqrt(self.head_dim)  # [batch_size, num_heads, seq_len, seq_len]\n",
    "\n",
    "        # 应用滑动窗口注意力掩码\n",
    "        if mask is None:\n",
    "            mask = torch.ones(seq_len, seq_len, device=x.device, dtype=torch.bool)\n",
    "            for i in range(seq_len):\n",
    "                start = max(0, i - self.window_size + 1)\n",
    "                mask[i, :start] = False\n",
    "        attn_weights = attn_weights.masked_fill(~mask, -1e18)\n",
    "\n",
    "        # 应用 attention sink 偏置\n",
    "        m_i = torch.max(torch.max(attn_weights, dim=-1, keepdim=True)[0], self.sink_bias)\n",
    "        attn_weights = attn_weights - m_i\n",
    "        attn_weights = torch.exp(attn_weights)\n",
    "        sink_term = torch.exp(self.sink_bias - m_i)\n",
    "        attn_weights = attn_weights / (attn_weights.sum(dim=-1, keepdim=True) + sink_term)\n",
    "\n",
    "        # 注意力输出\n",
    "        out = torch.matmul(attn_weights, v)\n",
    "        out = out.transpose(1, 2).reshape(batch_size, seq_len, dim)\n",
    "        out = self.o_proj(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "# GlobalAttention\n",
    "class GlobalAttention(SlidingWindowAttention):\n",
    "    \"\"\"全局注意力(GA) - 滑动窗口设为序列长度\"\"\"\n",
    "    def __init__(self, dim: int, num_heads: int = 64, num_kv_heads: int = 4):\n",
    "        super().__init__(dim, num_heads, num_kv_heads, window_size=1000000)  # 超大窗口模拟全局注意力"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b20af134",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SWABlock\n",
    "class SWABlock(nn.Module):\n",
    "    \"\"\"滑动窗口注意力块(SWA + MoE FFN)\"\"\"\n",
    "    def __init__(self, dim: int):\n",
    "        super().__init__()\n",
    "        self.norm1 = RMSNorm(dim)\n",
    "        self.attn = SlidingWindowAttention(dim)\n",
    "        self.norm2 = RMSNorm(dim)\n",
    "        self.ffn = MoEFFN(dim)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, mask: torch.Tensor = None) -> torch.Tensor:\n",
    "        x = x + self.attn(self.norm1(x), mask)\n",
    "        x = x + self.ffn(self.norm2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "16ae301f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GABlock\n",
    "class GABlock(nn.Module):\n",
    "    \"\"\"全局注意力块(GA + MoE FFN)\"\"\"\n",
    "    def __init__(self, dim: int, use_dense_ffn: bool = False):\n",
    "        super().__init__()\n",
    "        self.norm1 = RMSNorm(dim)\n",
    "        self.attn = GlobalAttention(dim)\n",
    "        self.norm2 = RMSNorm(dim)\n",
    "        # 第一个块使用稠密 FFN，其余使用 MoE FFN\n",
    "        if use_dense_ffn:\n",
    "            self.ffn = nn.Sequential(\n",
    "                nn.Linear(dim, 16384),\n",
    "                nn.SiLU(),\n",
    "                nn.Linear(16384, dim)\n",
    "            )\n",
    "        else:\n",
    "            self.ffn = MoEFFN(dim)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, mask: torch.Tensor = None) -> torch.Tensor:\n",
    "        x = x + self.attn(self.norm1(x), mask)\n",
    "        x = x + self.ffn(self.norm2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "72335b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MTP\n",
    "class MTPBlock(nn.Module):\n",
    "    \"\"\"多任务预测块(MTP)\"\"\"\n",
    "    def __init__(self, dim: int):\n",
    "        super().__init__()\n",
    "        self.norm1 = RMSNorm(dim)\n",
    "        self.attn = SlidingWindowAttention()\n",
    "        self.norm2 = RMSNorm(dim)\n",
    "        # 稠密 FFN（轻量设计)\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(dim, 1024),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(1024, dim)\n",
    "        )\n",
    "        # MTP 预测头\n",
    "        self.predict_head = nn.Linear(dim, dim)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor, mask: torch.Tensor = None) -> torch.Tensor:\n",
    "        x = x + self.attn(self.norm1(x), mask)\n",
    "        x = x + self.ffn(self.norm2(x))\n",
    "        x = self.predict_head(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dbcc3fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MiMoV2Flash\n",
    "class MiMoV2Flash(nn.Module):\n",
    "    \"\"\"MiMoV2Flash 模型\"\"\"\n",
    "    def __init__(self, vocab_size: int = 32000, dim: int = 4096, num_hybrid_blocks: int = 8):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        # 嵌入层\n",
    "        self.embeddings = nn.Embedding(vocab_size, dim)\n",
    "        # RoPE 位置编码\n",
    "        self.rope = RoPE(dim)\n",
    "        # 模型主结构: 1 个 GA 块(稠密 FFN) + 混合块(5 个 SWA + 1 个 GA)\n",
    "        self.layers = nn.ModuleList()\n",
    "        # 第一个块：GA + 稠密 FFN\n",
    "        self.layers.append(GABlock(dim, use_dense_ffn=True))\n",
    "        # 混合块：5 个 SWA + 1 个 GA\n",
    "        for _ in range(num_hybrid_blocks):\n",
    "            self.layers.append([SWABlock(dim) for _ in range(5)])\n",
    "            self.layers.append(GABlock[dim])\n",
    "        # 输出归一化\n",
    "        self.norm = RMSNorm(dim)\n",
    "        # 语言模型头\n",
    "        self.lm_head = nn.Linear(dim, vocab_size)\n",
    "        # 共享权重\n",
    "        self.lm_head.weight = self.embeddings.weight\n",
    "        # 3 层 MTP 模块\n",
    "        self.mtp_layers = nn.ModuleList(MTPBlock(dim) for _ in range(3))\n",
    "\n",
    "    def forward(self, input_ids: torch.Tensor, use_mtp: bool = False, mask: torch.Tensor = None) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input_ids: 输出 token ID，形状为 (batch_size, seq_len)\n",
    "            use_mtp: 是否使用 MTP 模块\n",
    "            mask: 掩码，形状为 (batch_size, seq_len)\n",
    "        \"\"\"\n",
    "        x = self.embeddings(input_ids)\n",
    "        x = self.rope(x)\n",
    "        \n",
    "        # 主模型前向传播\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask)\n",
    "        lm_logits = self.lm_head(self.norm(x))\n",
    "        \n",
    "        # MTP 模块生成 draft tokens\n",
    "        mtp_logits = None\n",
    "        if use_mtp:\n",
    "            mtp_x = x\n",
    "            for mtp_layer in self.mtp_layers:\n",
    "                mtp_x = mtp_layer(mtp_x, mask)\n",
    "            mtp_logits = self.lm_head(self.norm(mtp_x))\n",
    "\n",
    "        return {\"lm_logits\": lm_logits, \"mtp_logits\": mtp_logits}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fad2a1ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "使用设备: cuda\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "list is not a Module subclass",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m使用设备: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdevice\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# 初始化模型（简化版，减少专家数量以降低显存占用）\u001b[39;00m\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# 注：实际部署时可恢复num_experts=256\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m model = \u001b[43mMiMoV2Flash\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvocab_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m32000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m4096\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_hybrid_blocks\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m8\u001b[39;49m\u001b[43m)\u001b[49m.to(device)\n\u001b[32m     10\u001b[39m model.moe_ffn.num_experts = \u001b[32m32\u001b[39m  \u001b[38;5;66;03m# 简化：32个专家（原256）\u001b[39;00m\n\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# 生成测试\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 17\u001b[39m, in \u001b[36mMiMoV2Flash.__init__\u001b[39m\u001b[34m(self, vocab_size, dim, num_hybrid_blocks)\u001b[39m\n\u001b[32m     15\u001b[39m \u001b[38;5;66;03m# 混合块：5 个 SWA + 1 个 GA\u001b[39;00m\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_hybrid_blocks):\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlayers\u001b[49m\u001b[43m.\u001b[49m\u001b[43mappend\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mSWABlock\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdim\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[32;43m5\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     18\u001b[39m     \u001b[38;5;28mself\u001b[39m.layers.append(GABlock[dim])\n\u001b[32m     19\u001b[39m \u001b[38;5;66;03m# 输出归一化\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/ai/shenwei/workspace/codes/ModelServe/.venv/lib/python3.12/site-packages/torch/nn/modules/container.py:471\u001b[39m, in \u001b[36mModuleList.append\u001b[39m\u001b[34m(self, module)\u001b[39m\n\u001b[32m    465\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mappend\u001b[39m(\u001b[38;5;28mself\u001b[39m, module: Module) -> Self:\n\u001b[32m    466\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"Append a given module to the end of the list.\u001b[39;00m\n\u001b[32m    467\u001b[39m \n\u001b[32m    468\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m    469\u001b[39m \u001b[33;03m        module (nn.Module): module to append\u001b[39;00m\n\u001b[32m    470\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m471\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43madd_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    472\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/ai/shenwei/workspace/codes/ModelServe/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:650\u001b[39m, in \u001b[36mModule.add_module\u001b[39m\u001b[34m(self, name, module)\u001b[39m\n\u001b[32m    640\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"Add a child module to the current module.\u001b[39;00m\n\u001b[32m    641\u001b[39m \n\u001b[32m    642\u001b[39m \u001b[33;03mThe module can be accessed as an attribute using the given name.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    647\u001b[39m \u001b[33;03m    module (Module): child module to be added to the module.\u001b[39;00m\n\u001b[32m    648\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    649\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(module, Module) \u001b[38;5;129;01mand\u001b[39;00m module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m650\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtorch.typename(module)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m is not a Module subclass\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    651\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(name, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m    652\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[32m    653\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mmodule name should be a string. Got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtorch.typename(name)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    654\u001b[39m     )\n",
      "\u001b[31mTypeError\u001b[39m: list is not a Module subclass"
     ]
    }
   ],
   "source": [
    "# ------------------------------\n",
    "# 测试代码\n",
    "# ------------------------------\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"使用设备: {device}\")\n",
    "\n",
    "# 初始化模型（简化版，减少专家数量以降低显存占用）\n",
    "# 注：实际部署时可恢复num_experts=256\n",
    "model = MiMoV2Flash(vocab_size=32000, dim=4096, num_hybrid_blocks=8).to(device)\n",
    "target_num_experts = 32\n",
    "for layer in model.layers:\n",
    "    # 判断当前层是否包含 MoEFFN（SWABlock 和 GABlock 都有 ffn 属性）\n",
    "    if hasattr(layer, \"ffn\") and isinstance(layer.ffn, MoEFFN):\n",
    "        layer.ffn.num_experts = target_num_experts\n",
    "        # 重新初始化专家网络（可选，确保参数适配新的专家数量）\n",
    "        layer.ffn.experts = nn.ModuleList([Expert(layer.ffn.dim) for _ in range(target_num_experts)])\n",
    "\n",
    "# 生成测试\n",
    "batch_size = 2\n",
    "seq_len = 128\n",
    "input_ids = torch.randint(0, 32000, (batch_size, seq_len), device=device)\n",
    "\n",
    "# 前向传播(开启 FP16混合精度加速)\n",
    "with torch.autocast(device_type=device.type, dtype=torch.float16):\n",
    "    output = model(input_ids, use_mtp=True)\n",
    "\n",
    "# 输出结果验证\n",
    "print(f\"输入形状: {input_ids.shape}\")\n",
    "print(f\"LM输出形状: {output['lm_logits'].shape}\")\n",
    "print(f\"MTP输出形状: {output['mtp_logits'].shape}\")\n",
    "print(f\"模型参数量: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "\n",
    "# 计算损失（模拟训练）\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "lm_loss = loss_fn(output[\"lm_logits\"].reshape(-1, 32000), input_ids[:, 1:].reshape(-1))\n",
    "mtp_loss = loss_fn(output[\"mtp_logits\"].reshape(-1, 32000), input_ids[:, 1:].reshape(-1))\n",
    "total_loss = lm_loss + 0.1 * mtp_loss  # MTP损失权重0.1\n",
    "total_loss.backward()\n",
    "\n",
    "print(f\"LM损失: {lm_loss.item():.4f}\")\n",
    "print(f\"MTP损失: {mtp_loss.item():.4f}\")\n",
    "print(\"模型运行成功！\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0200c29d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ModelServe (3.12.12)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
