{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6d35c847",
   "metadata": {},
   "source": [
    "1. MLA(Multi-Label Attention): 选 MQA 模式，让所有查询头共享同一组 KV 向量，减少计算量（核心：共享 KV，适配 DSA 的高效需求）\n",
    "2. DSA(Deep Seek Attention): 拆成两步（核心：少算无用 token，降复杂度）\n",
    "    - 闪电索引器(Lightning Indexer): 用简单线性层快速给“查询-历史 Token\" 打分；\n",
    "    - Top-k 筛选: 只保留高分的 2048 个 Token 做注意力计算\n",
    "3. 结合：MLA 提供 KV 共享的基础框架，DSA 在 MLA 之上做“稀疏筛选“，最终注意力计算只针对 Top-k 的 KV 对"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "745ff1ea",
   "metadata": {},
   "source": [
    "**实现 MLA**  \n",
    "MLA 的 MQA 模式核心是 “单组 KV 供所有查询头使用”，避免重复计算 KV，适配 DSA 的稀疏逻辑。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "21ff8f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class MLA_MQA(nn.Module):\n",
    "    def __init__(self, d_model=512, num_query_heads=8, d_k=64):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.num_query_heads = num_query_heads\n",
    "        self.d_k = d_k\n",
    "\n",
    "        # 1. 查询投影（多查询头）：将输入h_t投影成 num_query_heads个查询\n",
    "        self.W_q = nn.Linear(d_model, num_query_heads * d_k)\n",
    "        # 2. KV 投影（单组， MQA 模式）：将所有查询头共享这组 KV\n",
    "        self.W_k = nn.Linear(d_model, d_k)\n",
    "        self.W_v = nn.Linear(d_model, d_k)\n",
    "        # 3. 输出投影\n",
    "        self.W_o = nn.Linear(num_query_heads * d_k, d_model)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x: [batch_size, seq_len, d_model] 输入序列\n",
    "        batch_size, seq_len, _ = x.shape\n",
    "\n",
    "        # 生成查询 Q：[batch_size, num_query_heads, seq_len, d_k]\n",
    "        Q = self.W_q(x).view(batch_size, seq_len, self.num_query_heads, self.d_k).transpose(1, 2)\n",
    "        # 生成共享 KV：[batch_size, seq_len, d_k]\n",
    "        K = self.W_k(x)  # [batch_size, seq_len, d_k]\n",
    "        V = self.W_v(x)  # [batch_size, seq_len, d_k]\n",
    "\n",
    "        return Q, K, V  # 输出Q（多头）、共享 KV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee3e1306",
   "metadata": {},
   "source": [
    "**实现 DSA**  \n",
    "基于 MLA 的 KV，用“闪电索引器打分 + Top-k 筛选“实现稀疏计算，核心是“只关注高分 Token“"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "06761f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DSA(nn.Module):\n",
    "    def __init__(self, d_model=512, d_k=64, top_k=2048):\n",
    "        super().__init__()\n",
    "        self.top_k = top_k  # 每个查询保留 Top-k Token（原文默认：2048）\n",
    "        self.d_k = d_k\n",
    "\n",
    "        # 闪电索引器（简化版）： 快速计算查询与历史 Token 的相关性得分\n",
    "        self.indexer_q = nn.Linear(d_model, d_k)  # 查询投影到索引器维度\n",
    "        self.indexer_k = nn.Linear(d_model, d_k)  # 历史 Token 投影到索引器维度\n",
    "        self.relu = nn.ReLU()  # 原文使用 ReLU 提升吞吐量\n",
    "\n",
    "    def forward(self, x, Q, K, V):\n",
    "        # x: [batch_size, seq_len, d_model] 原始输入（用于索引器打分）\n",
    "        # Q: [batch_size, num_query_heads, seq_len, d_k] （MLA 输出的查询）\n",
    "        # K，V: [batch_size, seq_len, d_k] （MLA 输出的共享 KV）\n",
    "        batch_size, seq_len, _ = x.shape\n",
    "        num_query_heads = Q.shape[1]\n",
    "\n",
    "        # 1. 闪电索引器：计算每个查询 Token 与所有历史 Token 的得分I_{t,s}\n",
    "        q_index = self.indexer_q(x)  # [batch_size, seq_len, d_k]\n",
    "        k_index = self.indexer_k(x)  # [batch_size, seq_len, d_k]\n",
    "        # 得分计算 q_index @ k_index.T --> [batch_size, seq_len, seq_len]（每个位置对所有位置的得分）\n",
    "        index_scores = torch.bmm(q_index, k_index.transpose(1, 2))  # I_{t,s}\n",
    "        index_scores = self.relu(index_scores)  # 激活函数\n",
    "\n",
    "        # 2. Top-k 筛选： 每个查询 token 只保留得分最高的 top-k 个历史 token\n",
    "        # top_k_values: 得分值，top_k_indices: 得分对应的位置索引\n",
    "        top_k_values, top_k_indices = torch.topk(index_scores, k=min(self.top_k, seq_len), dim=-1)\n",
    "        # top_k_indices: [batch_size, seq_len, top_k] （每个查询 token 对应的 top-k 个历史 token 索引）\n",
    "\n",
    "        # 3. 提取 Top-k 对应的 KV 向量（共享 KV，所以直接按索引取）\n",
    "        # 先把索引展平，方便批量提取\n",
    "        batch_idx = torch.arange(batch_size).unsqueeze(-1).unsqueeze(-1).repeat(1, seq_len, self.top_k)\n",
    "        # K_topk: [batch_size, seq_len, top_k, d_k]\n",
    "        K_topk = K[batch_idx, top_k_indices]\n",
    "        V_topk = V[batch_idx, top_k_indices]\n",
    "\n",
    "        # 4. 稀疏注意力计算\n",
    "        # 调整 Q 维度: [batch_size, num_query_heads, seq_len, d_k] --> [batch_size, seq_len, num_query_heads, d_k]\n",
    "        Q_reshaped = Q.transpose(1, 2)\n",
    "        # 调整K—topk 维度：[batch_size, seq_len, top_k, d_k] --> [batch_size, seq_len, d_k, top_k]\n",
    "        K_topk_T = K_topk.transpose(-1, -2)\n",
    "        # 注意力得分 Q @ K.T / (sqrt(d_k) -> [batch_size, seq_len, num_query_heads, top_k]\n",
    "        attn_scores = torch.matmul(Q_reshaped, K_topk_T) / torch.sqrt(torch.tensor(self.d_k, dtype=torch.float32))\n",
    "        # softmax 归一化\n",
    "        attn_weights = F.softmax(attn_scores, dim=-1)\n",
    "        # 注意力输出 权重 @ V --> [batch_size, seq_len, num_query_heads, d_k]\n",
    "        attn_output = torch.matmul(attn_weights, V_topk)\n",
    "\n",
    "        # 5. 重组输出：[batch_size, seq_len, num_query_heads * d_k] --> [batch_size, seq_len, d_model]\n",
    "        attn_output = attn_output.reshape(batch_size, seq_len, num_query_heads * self.d_k)\n",
    "        return attn_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9f3824d",
   "metadata": {},
   "source": [
    "**结合 DSA+MLA**  \n",
    "MLA 提供共享 KV，DSA 做稀疏筛选和注意力计算，最后通过 MLA 的输出投影得到结果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "26aede4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DSA_MLA(nn.Module):\n",
    "    def __init__(self, d_model=512, num_query_heads=8, d_k=64, top_k=2048):\n",
    "        super().__init__()\n",
    "        self.mla_mqa = MLA_MQA(d_model=d_model, num_query_heads=num_query_heads, d_k=d_k)\n",
    "        self.dsa = DSA(d_model=d_model, d_k=d_k, top_k=top_k)\n",
    "        self.W_o = nn.Linear(num_query_heads * d_k, d_model)  # 最终输出投影（和 MLA 复用也可）\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [batch_size, seq_len, d_model] 输入序列\n",
    "        # 1. MLQ 生成共享 KV 和多查询头Q\n",
    "        Q, K, V = self.mla_mqa(x)\n",
    "        # 2. DSA 稀疏计算\n",
    "        dsa_output = self.dsa(x, Q, K, V)\n",
    "        # 3. 输出投影\n",
    "        output = self.W_o(dsa_output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d7cd0d3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "输入维度: torch.Size([2, 100, 512])\n",
      "输出维度: torch.Size([2, 100, 512])\n",
      "模型运行成功！\n"
     ]
    }
   ],
   "source": [
    "d_model = 512\n",
    "num_query_heads = 8\n",
    "d_k = 64\n",
    "top_k = 10\n",
    "batch_size = 2\n",
    "seq_len = 100\n",
    "\n",
    "x = torch.randn(batch_size, seq_len, d_model)\n",
    "\n",
    "model = DSA_MLA(d_model=d_model, num_query_heads=num_query_heads, d_k=d_k, top_k=top_k)\n",
    "\n",
    "output = model(x)\n",
    "\n",
    "# 验证输出维度（应和输入维度一致）\n",
    "print(f\"输入维度: {x.shape}\")  # torch.Size([2, 100, 512])\n",
    "print(f\"输出维度: {output.shape}\")  # torch.Size([2, 100, 512])\n",
    "print(\"模型运行成功！\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ModelServe (3.12.12)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
