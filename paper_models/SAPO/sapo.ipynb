{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "15dd692c",
   "metadata": {},
   "source": [
    "![SAPO](./image.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ed0e611",
   "metadata": {},
   "source": [
    "* **SAPO算法**的核心思路\n",
    "    1. 用简单的 MLP 策略网络模拟 LLM 的 token 生成逻辑\n",
    "    2. 实现 SAPO 的温度可控软门控（替代硬裁剪）\n",
    "    3. 实现正负 token 的非对称温度（τ_neg > τ_pos）\n",
    "    4. 用模拟的文本生成任务（固定目标序列）验证算法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fde5c418",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from torch.distributions import Categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "639584cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 参数\n",
    "VOCAB_SIZE = 10  # 词汇表大小\n",
    "SEQ_LEN = 5  # 序列长度\n",
    "BATCH_SIZE = 8  # 批量大小\n",
    "GRADIENT_STEPS = 100  # 训练步数\n",
    "LR = 1e-3  # 学习率\n",
    "TAU_POS = 1.0 # 正 advantage 的温度\n",
    "TAU_NEG = 1.05 # 负 advantage 的温度(> TAU_POS)\n",
    "GAMMA = 0.9 # 折扣因子(用于优势计算)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "af354677",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimplePolicy(nn.Module):\n",
    "    def __init__(self, vocab_size, seq_len, hidden_dim=32):\n",
    "        super(SimplePolicy, self).__init__()\n",
    "        self.seq_len = seq_len\n",
    "        self.vocab_size = vocab_size\n",
    "        self.fc1 = nn.Linear(seq_len, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, seq_len * vocab_size)\n",
    "        self.relu = nn.ReLU()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        logits = self.fc2(x)\n",
    "        logits = logits.reshape(-1, self.seq_len, self.vocab_size)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "372d6b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 简化的优势函数计算：归一化奖励（论文中 group-normalized advantage）\n",
    "def compute_advantage(rewards, batch_rewards):\n",
    "    adv = rewards - batch_rewards.mean()\n",
    "    return adv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f2380074",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAPO核心：温度可控的软门控函数\n",
    "# 基于论文公式σ(x)(1-σ(x)) = 1/4 * sech²(x/2)，实现平滑衰减\n",
    "def sigmoid_gate(r_t, tau):\n",
    "    x = (r_t - 1) * tau  # 偏离 on-policy(r_t=1)的程度\n",
    "    sig = torch.sigmoid(x)\n",
    "    gate_weight = 4 / tau * sig * (1 - sig)  # 论文中的4/τ因子\n",
    "    return gate_weight.clamp(0, 1)  # 限制权重范围"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d72bb139",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sapo_loss(old_policy, new_policy, states, actions, rewards, tau_pos, tau_neg):\n",
    "    \"\"\"\n",
    "    计算SAPO的损失函数\n",
    "    :param old_policy: 旧策略（行为策略）\n",
    "    :param new_policy: 新策略（目标策略）\n",
    "    :param states: 输入状态\n",
    "    :param actions: 采样的token序列\n",
    "    :param rewards: 序列奖励\n",
    "    :param tau_pos: 正advantage温度\n",
    "    :param tau_neg: 负advantage温度\n",
    "    :return: SAPO损失\n",
    "    \"\"\"\n",
    "    # 1. 计算 tokens 级 importance ratio r_t(θ)\n",
    "    old_logits = old_policy(states)\n",
    "    new_logits = new_policy(states)\n",
    "\n",
    "    old_dist = Categorical(logits=old_logits)\n",
    "    new_dist = Categorical(logits=new_logits)\n",
    "\n",
    "    # token 级的对数概率(shape: [batch, seq_len])\n",
    "    old_log_probs = old_dist.log_prob(actions)\n",
    "    new_log_probs = new_dist.log_prob(actions)\n",
    "\n",
    "    # importance ratio r_t = π_new / π_old （token级）\n",
    "    r_t = torch.exp(new_log_probs - old_log_probs)  # [batch, seq_len]\n",
    "\n",
    "    # 2. 计算 group-normalized advantage\n",
    "    batch_adv = compute_advantage(rewards, rewards)  # [batch]\n",
    "    token_adv = batch_adv.unsqueeze(1).repeat(1, SEQ_LEN)  # [batch, seq_len]\n",
    "\n",
    "    # 3. 非对称温度：根据 advantage 正负选择温度\n",
    "    tau = torch.where(token_adv > 0, tau_pos, tau_neg)  # [batch, seq_len]\n",
    "\n",
    "    # 4. 计算软门控权重\n",
    "    gate_weights = sigmoid_gate(r_t, tau)  # [batch, seq_len]\n",
    "\n",
    "    # 5. SAPO目标：加权 policy gradient\n",
    "    # 目标最大化，因此损失去负值\n",
    "    sapo_obj = gate_weights * r_t * token_adv\n",
    "    loss = -sapo_obj.mean()\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82e60a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_reward(actions, target_seq):\n",
    "    \"\"\"模拟奖励：与目标序列的匹配度越高，奖励越高\"\"\"\n",
    "    # target_seq: 目标序列(shape: [batch_size, seq_len])\n",
    "    target_seq_batch = target_seq.unsqueeze(0).repeat(actions.shape[0], 1)\n",
    "    match = (actions == target_seq_batch).float()\n",
    "    seq_reward = match.sum(dim=1) / SEQ_LEN  # [batch]\n",
    "    return seq_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1542c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 运行\n",
    "# 1. 初始化策略网络（新策略+旧策略）\n",
    "policy = SimplePolicy(vocab_size=VOCAB_SIZE, seq_len=SEQ_LEN)\n",
    "old_policy = SimplePolicy(vocab_size=VOCAB_SIZE, seq_len=SEQ_LEN)\n",
    "old_policy.load_state_dict(policy.state_dict())  # 初始时新旧策略一致\n",
    "optimizer = optim.Adam(policy.parameters(), lr=LR)\n",
    "\n",
    "# 2. 定义目标序列（模拟任务：让模型生成该序列）\n",
    "target_seq = torch.tensor([2, 5, 7, 3, 1])  # 固定目标token\n",
    "\n",
    "# 3. 训练循环\n",
    "for step in range(GRADIENT_STEPS):\n",
    "    # 3.1 采样数据（模拟 rollout 过程）\n",
    "    # 生成随机状态（模拟输入query的特征）\n",
    "    states = torch.randn(BATCH_SIZE, SEQ_LEN)\n",
    "\n",
    "    # 旧策略采样 token 序列\n",
    "    with torch.no_grad():\n",
    "        old_logits = policy(states)\n",
    "        old_dist = Categorical(logits=old_logits)\n",
    "        actions = old_dist.sample()  # [batch, seq_len]\n",
    "    \n",
    "    # 计算序列奖励\n",
    "    rewards = simulate_reward(actions, target_seq)\n",
    "\n",
    "    # 3.2 计算 SAPO 损失\n",
    "    loss = sapo_loss(\n",
    "        old_policy=old_policy,\n",
    "        new_policy=policy,\n",
    "        states=states,\n",
    "        actions=actions,\n",
    "        rewards=rewards,\n",
    "        tau_pos=TAU_POS,\n",
    "        tau_neg=TAU_NEG\n",
    "    )\n",
    "\n",
    "    # 3.3 更新策略\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # 3.4 更新旧策略\n",
    "    if (step + 1) % 10 == 0:\n",
    "        print(\"\\n===== 更新旧策略 =====\")\n",
    "        old_policy.load_state_dict(policy.state_dict())\n",
    "\n",
    "    # 3.5 打印训练日志\n",
    "    if (step + 1) % 10 == 0:\n",
    "        avg_reward = rewards.mean().item()\n",
    "        print(f\"Step [{step+1}/{GRADIENT_STEPS}] | Loss: {loss.item():.4f} | Avg Reward: {avg_reward:.4f}\")\n",
    "\n",
    "# 训练完成后验证\n",
    "print(\"\\n===== 训练完成，验证策略 =======\")\n",
    "test_state = torch.randn(1, SEQ_LEN)\n",
    "with torch.no_grad():\n",
    "    logits = policy(test_state)\n",
    "    dist = Categorical(logits=logits)\n",
    "    gen_seq = dist.sample().squeeze(0)\n",
    "print(f\"目标序列: {target_seq.numpy()}\")\n",
    "print(f\"生成序列: {gen_seq.numpy()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d63856b5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ModelServe (3.12.12)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
